{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay, ExponentialDecay, CosineDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras_tuner as kt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('/home/ryanmichaud/common_repo/Muon_Collider_Smart_Pixels/MuC_Smartpix_ML/')\n",
    "from Model_Classes import SmartPixModel\n",
    "from OptimizedDataGenerator4 import *\n",
    "from qkeras import QDense, QActivation, quantized_bits, quantized_relu\n",
    "\n",
    "\n",
    "\n",
    "class Model1(SmartPixModel):\n",
    "    def __init__(self,\n",
    "            tfRecordFolder: str = \"/local/d1/smartpixML/filtering_models/shuffling_data/all_batches_shuffled_bigData_try2/filtering_records16384_data_shuffled_single_bigData/\",\n",
    "            nBits: list = None, # just for fractional bits, integer bits\n",
    "                                ## number of bits is the number of bits for each quantized model and then\n",
    "                                ## run training should make one model for each bit size\n",
    "            loadModel: bool = False,\n",
    "            modelPath: str = None, # Only include if you are loading a model\n",
    "            ): \n",
    "        self.tfRecordFolder = tfRecordFolder\n",
    "        self.modelName = \"Model 1\" # for other models, e.g., Model 1, Model 2, etc.\n",
    "        self.model = None\n",
    "        self.unquantized_history = None\n",
    "\n",
    "        self.quantized_model = None\n",
    "        self.quantized_history = None\n",
    "        self.hyperparameterModel = None\n",
    "        self.trainODG = None\n",
    "        self.validationODG = None\n",
    "        return\n",
    "    \n",
    "    def runAllStuff(self,):\n",
    "        return\n",
    "    \n",
    "    def loadTfRecords(self):\n",
    "        # Load the TFRecords using the OptimizedDataGenerator4\n",
    "        validation_dir = \"./tf_records1000Daniel/tfrecords_validation/\"\n",
    "        train_dir = \"./tf_records1000Daniel/tfrecords_train/\"\n",
    "        x_feature_description: list = ['x_size','z_global','y_profile','x_profile','cluster', 'y_size', 'x_size', 'y_local']\n",
    "        self.trainODG = OptimizedDataGenerator(tf_records_dir=train_dir,load_records=True, x_feature_description=x_feature_description)\n",
    "        self.validationODG = OptimizedDataGenerator(tf_records_dir=validation_dir,load_records=True, x_feature_description=x_feature_description)\n",
    "\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def makeUnquantizedModel(self):\n",
    "        ## here i will be making a 4-layer neural network \n",
    "        ## Model 1: z-global, x size, y size, y local\n",
    "\n",
    "\n",
    "        ## define the inputs\n",
    "        input1 = tf.keras.layers.Input(shape=(1,), name=\"z_global\")\n",
    "        input2 = tf.keras.layers.Input(shape=(1,), name=\"x_size\")\n",
    "        input3 = tf.keras.layers.Input(shape=(1,), name=\"y_size\")\n",
    "        input4 = tf.keras.layers.Input(shape=(1,), name=\"y_local\")\n",
    "\n",
    "        ## concatenate the inputs into one layer\n",
    "        inputList = [input1, input2, input3, input4]\n",
    "        inputs = tf.keras.layers.Concatenate()(inputList)\n",
    "\n",
    "\n",
    "        ## here i will add the layers \n",
    "\n",
    "        stack1 = tf.keras.layers.Dense(17,activation='relu')(inputs)\n",
    "        stack2 = tf.keras.layers.Dense(20, activation='relu')(stack1)\n",
    "        stack3 = tf.keras.layers.Dense(9, activation='relu')(stack2)\n",
    "        stack4 = tf.keras.layers.Dense(16, activation='relu')(stack3)\n",
    "        stack5 = tf.keras.layers.Dense(18, activation='relu')(stack4)\n",
    "        output = tf.keras.layers.Dense(1,activation='sigmoid')(stack5)\n",
    "\n",
    "        self.model = tf.keras.Model(inputs=inputList, outputs=output)\n",
    "\n",
    "\n",
    "    def makeUnquatizedModelHyperParameterTuning(self):\n",
    "        def model_builder(hp):\n",
    "            # ── B) Architecture hyperparams ──────────────────────────────────────────\n",
    "            # separately tune rows and cols\n",
    "\n",
    "            row1nodes      = hp.Int(\"1\",   10, 200, step=10)\n",
    "            row2nodes      = hp.Int(\"2\",   10, 200, step=10)\n",
    "            row3nodes      = hp.Int(\"3\",   10, 200, step=10)\n",
    "            row4nodes      = hp.Int(\"4\",   10, 200, step=10)\n",
    "            row5nodes      = hp.Int(\"5\",   10, 200, step=10)\n",
    "\n",
    "\n",
    "\n",
    "            input1 = tf.keras.layers.Input(shape=(1,), name=\"z_global\")\n",
    "            input2 = tf.keras.layers.Input(shape=(1,), name=\"x_size\")\n",
    "            input3 = tf.keras.layers.Input(shape=(1,), name=\"y_size\")\n",
    "            input4 = tf.keras.layers.Input(shape=(1,), name=\"y_local\")\n",
    "\n",
    "            ## concatenate the inputs into one layer\n",
    "            inputList = [input1, input2, input3, input4]\n",
    "            inputs = tf.keras.layers.Concatenate()(inputList)\n",
    "\n",
    "\n",
    "            ## here i will add the layers \n",
    "\n",
    "            # layer 1\n",
    "            x = tf.keras.layers.Dense(row1nodes,activation='relu')(inputs)\n",
    "            x = tf.keras.layers.Dense(row2nodes, activation='relu')(x)\n",
    "            x = tf.keras.layers.Dense(row3nodes, activation='relu')(x)\n",
    "            x = tf.keras.layers.Dense(row4nodes, activation='relu')(x)\n",
    "            x = tf.keras.layers.Dense(row5nodes, activation='relu')(x)\n",
    "            output = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "            model = tf.keras.Model(inputs=inputList, outputs=output)\n",
    "\n",
    "            model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"binary_accuracy\"],\n",
    "            run_eagerly  = True \n",
    "            )\n",
    "            return model\n",
    "\n",
    "        tuner = kt.RandomSearch(\n",
    "        model_builder,\n",
    "        objective           = \"val_binary_accuracy\",\n",
    "        max_trials          = 120,\n",
    "        executions_per_trial = 2,\n",
    "        project_name        = \"new_hyperparam_search\"\n",
    "        )\n",
    "\n",
    "        tuner.search(\n",
    "            trainODG,\n",
    "            validation_data = self.validationODG,\n",
    "            epochs          = 110,\n",
    "            callbacks       = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            ]\n",
    "        )\n",
    "     \n",
    "\n",
    "\n",
    "    \n",
    "    def makeQuantizedModel(self):\n",
    "        \"\"\"\n",
    "        Build & compile your QKeras model with the given number of integer bits.\n",
    "        \"\"\"\n",
    "        tf.keras.backend.clear_session()\n",
    "        # inputs\n",
    "        input1 = tf.keras.layers.Input(shape=(1,), name=\"z_global\")\n",
    "        input2 = tf.keras.layers.Input(shape=(1,), name=\"x_size\")\n",
    "        input3 = tf.keras.layers.Input(shape=(1,), name=\"y_size\")\n",
    "        input4 = tf.keras.layers.Input(shape=(1,), name=\"y_local\")\n",
    "        x = tf.keras.layers.Concatenate()([input1, input2, input3, input4])\n",
    "\n",
    "        ## I want to try this with 1 int bit and 7 fractional\n",
    "        ## I want to try this with 0 int bit and 7 fractional\n",
    "        \n",
    "        # layer 1\n",
    "        x = QDense(\n",
    "            17,\n",
    "            kernel_quantizer=quantized_bits(8, 0, alpha=0.001),\n",
    "            bias_quantizer=quantized_bits(8, 0, alpha=0.001),\n",
    "            #kernel_regularizer=tf.keras.regularizers.L1L2(0.0001),\n",
    "            ## adds sum of the activations squared to the loss function \n",
    "            #activity_regularizer=tf.keras.regularizers.L2(0.0001),\n",
    "        )(x)\n",
    "        x = QActivation(\n",
    "            activation=quantized_relu(8, 0),\n",
    "            name=\"q_relu1\"\n",
    "        )(x)\n",
    "\n",
    "        # layer 2 (example—you can tweak per‐layer bits)\n",
    "        x = QDense(\n",
    "            20,\n",
    "            kernel_quantizer=quantized_bits(8, 0, alpha=0.001),\n",
    "            bias_quantizer=quantized_bits(8, 0, alpha=0.001),\n",
    "            #kernel_regularizer=tf.keras.regularizers.L1L2(0.0001),\n",
    "            ## adds sum of the activations squared to the loss function \n",
    "            #activity_regularizer=tf.keras.regularizers.L2(0.0001),\n",
    "        )(x)\n",
    "        x = QActivation(\n",
    "            activation=quantized_relu(8, 0),\n",
    "            name=\"q_relu2\"\n",
    "        )(x)\n",
    "\n",
    "        # layer 3\n",
    "        x = QDense(\n",
    "            9,\n",
    "            kernel_quantizer=quantized_bits(8, 0, alpha=0.001),\n",
    "            bias_quantizer=quantized_bits(8, 0, alpha=0.001),\n",
    "            #kernel_regularizer=tf.keras.regularizers.L1L2(0.0001),\n",
    "            ## adds sum of the activations squared to the loss function \n",
    "            #activity_regularizer=tf.keras.regularizers.L2(0.0001),\n",
    "        )(x)\n",
    "        x = QActivation(\n",
    "            activation=quantized_relu(8, 0),\n",
    "            name=\"q_relu3\"\n",
    "        )(x)\n",
    "\n",
    "        # layer 4\n",
    "        x = QDense(\n",
    "            16,\n",
    "            kernel_quantizer=quantized_bits(8, 0, alpha=0.001),\n",
    "            bias_quantizer=quantized_bits(8, 0, alpha=0.001),\n",
    "            #kernel_regularizer=tf.keras.regularizers.L1L2(0.0001),\n",
    "            ## adds sum of the activations squared to the loss function \n",
    "            #activity_regularizer=tf.keras.regularizers.L2(0.0001),\n",
    "        )(x)\n",
    "        x = QActivation(\n",
    "            activation=quantized_relu(8, 0),\n",
    "            name=\"q_relu4\"\n",
    "        )(x)\n",
    "\n",
    "        # layer 5\n",
    "        x = QDense(\n",
    "            8,\n",
    "            kernel_quantizer=quantized_bits(8, 0, alpha=0.001),\n",
    "            bias_quantizer=quantized_bits(8, 0, alpha=0.001),\n",
    "            #kernel_regularizer=tf.keras.regularizers.L1L2(0.0001),\n",
    "            ## adds sum of the activations squared to the loss function \n",
    "            #activity_regularizer=tf.keras.regularizers.L2(0.0001),\n",
    "        )(x)\n",
    "        x = QActivation(\n",
    "            activation=quantized_relu(8, 0),\n",
    "            name=\"q_relu5\"\n",
    "        )(x)\n",
    "\n",
    "        # output\n",
    "        x = QDense(\n",
    "            1,\n",
    "            kernel_quantizer=quantized_bits(8, 0, alpha=0.001),\n",
    "            bias_quantizer=quantized_bits(8, 0, alpha=0.001),\n",
    "            #kernel_regularizer=tf.keras.regularizers.L2(0.0001),\n",
    "        )(x)\n",
    "        out = QActivation(\"smooth_sigmoid\")(x)\n",
    "        self.quantized_model = tf.keras.Model(inputs=[input1, input2, input3, input4], outputs=out)\n",
    "\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def runHyperparameterTuning(self):\n",
    "        raise NotImplementedError(\"Subclasses should implement this method.\")\n",
    "    \"\"\"\n",
    "    \n",
    "    def buildModel(self):\n",
    "       self.makeUnquantizedModel()\n",
    "\n",
    "    def loadModel(self, file_path: str):\n",
    "        self.model=tf.keras.models.load_model(file_path, compile=False)\n",
    "\n",
    "    def saveModel(self, overwrite=False):\n",
    "        file_path = Path(f'./{self.modelName}.keras').resolve()\n",
    "        if not overwrite and os.path.exists(file_path):\n",
    "            raise Exception(\"Model exists. To overwrite existing saved model, set overwrite to True.\")\n",
    "        self.model.save(file_path)\n",
    "    \n",
    "    #dataset\n",
    "    def trainUnquantizedModel(self): # in the input, specify the learning rate scheduler, etc.\n",
    "        self.loadTfRecords()\n",
    "        callbacks = []\n",
    "        self.model.compile(optimizer='adam', \n",
    "                            loss='binary_crossentropy', \n",
    "                            metrics=['binary_accuracy'],\n",
    "                            run_eagerly=True\n",
    "                            )\n",
    "        self.model.fit(x=self.trainODG,validation_data=self.validationODG, callbacks=callbacks,epochs=100)\n",
    "\n",
    "\n",
    "    def trainQuantizedModel(self): # in the input, specify the learning rate scheduler, etc.\n",
    "        self.loadTfRecords()\n",
    "        callbacks = []\n",
    "        self.quantized_model.compile(optimizer='adam', \n",
    "                            loss='binary_crossentropy', \n",
    "                            metrics=['binary_accuracy'],\n",
    "                            run_eagerly=True\n",
    "                            )\n",
    "        self.quantized_history = self.quantized_model.fit(x=self.trainODG,validation_data=self.validationODG, callbacks=callbacks,epochs=100)\n",
    "\n",
    "    #plot based on history\n",
    "    def plotUnquantizedModel(self):\n",
    "        def plotModelHistory(history,modelNum = -999):\n",
    "            plt.subplot(211)\n",
    "            # Plot training & validation loss values\n",
    "            plt.plot(history.history['loss'],label=\"Train\")\n",
    "            plt.plot(history.history['val_loss'],label=\"Validation\")\n",
    "            plt.title(f'Model {modelNum} loss and accuracy')\n",
    "            plt.ylabel('Loss')\n",
    "            # plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "            plt.subplot(212)\n",
    "            # Plot training & validation accuracy values\n",
    "            plt.plot(history.history['binary_accuracy'],label=\"Train\")\n",
    "            plt.plot(history.history['val_binary_accuracy'],label=\"Validation\")\n",
    "            # plt.title(f'Model {modelNum} accuracy')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend(loc='upper left')\n",
    "            plt.show()\n",
    "        plotModelHistory(self.unquantized_history, 1)\n",
    "\n",
    "    def plotQuantizedModel(self):\n",
    "        def plotModelHistory(history,modelNum = -999):\n",
    "            plt.subplot(211)\n",
    "            # Plot training & validation loss values\n",
    "            plt.plot(history.history['loss'],label=\"Train\")\n",
    "            plt.plot(history.history['val_loss'],label=\"Validation\")\n",
    "            plt.title(f'Model {modelNum} loss and accuracy')\n",
    "            plt.ylabel('Loss')\n",
    "            # plt.xlabel('Epoch')\n",
    "            plt.legend()\n",
    "            plt.subplot(212)\n",
    "            # Plot training & validation accuracy values\n",
    "            plt.plot(history.history['binary_accuracy'],label=\"Train\")\n",
    "            plt.plot(history.history['val_binary_accuracy'],label=\"Validation\")\n",
    "            # plt.title(f'Model {modelNum} accuracy')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.legend(loc='upper left')\n",
    "            plt.show()\n",
    "        plotModelHistory(self.quantized_history, 1)\n",
    "\n",
    "\n",
    "    # Evaluate the model\n",
    "    def evaluate(self):\n",
    "        print(self.quantized_model.evaluate(self.validationODG, verbose=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n",
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryanmichaud/miniconda3/envs/mlproj_qkeras/lib/python3.10/site-packages/keras/src/engine/functional.py:642: UserWarning: Input dict contained keys ['y_profile', 'x_profile', 'cluster'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 5s 89ms/step - loss: 0.6931 - binary_accuracy: 0.4798 - val_loss: 0.6931 - val_binary_accuracy: 0.4445\n",
      "Epoch 2/100\n",
      "60/60 [==============================] - 5s 84ms/step - loss: 0.6931 - binary_accuracy: 0.3958 - val_loss: 0.6931 - val_binary_accuracy: 0.4445\n",
      "Epoch 3/100\n",
      "60/60 [==============================] - 5s 83ms/step - loss: 0.6931 - binary_accuracy: 0.4798 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 4/100\n",
      "60/60 [==============================] - 5s 83ms/step - loss: 0.6931 - binary_accuracy: 0.4462 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 5/100\n",
      "60/60 [==============================] - 5s 83ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 6/100\n",
      "60/60 [==============================] - 5s 83ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 7/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 8/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 9/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 10/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 11/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 12/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 13/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 14/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 15/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 16/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 17/100\n",
      "60/60 [==============================] - 5s 83ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 18/100\n",
      "60/60 [==============================] - 5s 83ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 19/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 20/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 21/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 22/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 23/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 24/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 25/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 26/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 27/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 28/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 29/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 30/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 31/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 32/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 33/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 34/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 35/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 36/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 37/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 38/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 39/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 40/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 41/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 42/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 43/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 44/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 45/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 46/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 47/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 48/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 49/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 50/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 51/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 52/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 53/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 54/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 55/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 56/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 57/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 58/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 59/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 60/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 61/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 62/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 63/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 64/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 65/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 66/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 67/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 68/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 69/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 70/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 71/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 72/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 73/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 74/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 75/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 76/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 77/100\n",
      "60/60 [==============================] - 5s 83ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 78/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 79/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 80/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 81/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 82/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 83/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 84/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 85/100\n",
      "60/60 [==============================] - 5s 82ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 86/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 87/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 88/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 89/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 90/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 91/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 92/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 93/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 94/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 95/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 96/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 97/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 98/100\n",
      "60/60 [==============================] - 5s 81ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 99/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n",
      "Epoch 100/100\n",
      "60/60 [==============================] - 5s 80ms/step - loss: 0.6931 - binary_accuracy: 0.5202 - val_loss: 0.6931 - val_binary_accuracy: 0.5555\n"
     ]
    }
   ],
   "source": [
    "m1 = Model1()\n",
    "\n",
    "m1.makeQuantizedModel()\n",
    "m1.trainQuantizedModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validationODG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mm1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[64], line 285\u001b[0m, in \u001b[0;36mModel1.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantized_model\u001b[38;5;241m.\u001b[39mevaluate(\u001b[43mvalidationODG\u001b[49m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'validationODG' is not defined"
     ]
    }
   ],
   "source": [
    "m1.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj_qkeras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
