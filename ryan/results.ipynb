{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe3e8a9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T16:30:48.630783Z",
     "iopub.status.busy": "2025-11-14T16:30:48.630460Z",
     "iopub.status.idle": "2025-11-14T16:30:51.422592Z",
     "shell.execute_reply": "2025-11-14T16:30:51.422165Z"
    },
    "papermill": {
     "duration": 2.798716,
     "end_time": "2025-11-14T16:30:51.423464",
     "exception": false,
     "start_time": "2025-11-14T16:30:48.624748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 12:21:11.554445: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-16 12:21:11.556924: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-16 12:21:11.589240: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-16 12:21:11.589261: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-16 12:21:11.589282: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-16 12:21:11.595208: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-16 12:21:11.595609: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-16 12:21:12.300355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'MuC_Smartpix_ML.Model_Classes.SmartPixModel'>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from OptimizedDataGenerator4 import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "noGPU=False\n",
    "if noGPU:\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "print(\"\\nHIIIIIIIIIIIIIIIIII\\n\")\n",
    "\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.test.is_built_with_gpu_support())\n",
    "print(tf.test.is_gpu_available())\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "# import all the necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay, ExponentialDecay, CosineDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras_tuner as kt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"/local/d1/smartpixML/filtering_models/shuffling_data/\") #TODO use the ODG from here\n",
    "import OptimizedDataGenerator4_data_shuffled_bigData as ODG2\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "sys.path.append(\"../ryan\")\n",
    "import OptimizedDataGenerator4 as ODG\n",
    "\n",
    "\n",
    "sys.path.append(str(Path.cwd().parents[0]))\n",
    "\n",
    "from MuC_Smartpix_ML.Model_Classes import SmartPixModel\n",
    "print(SmartPixModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2083aded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T16:30:51.427155Z",
     "iopub.status.busy": "2025-11-14T16:30:51.426764Z",
     "iopub.status.idle": "2025-11-14T16:30:51.430408Z",
     "shell.execute_reply": "2025-11-14T16:30:51.430051Z"
    },
    "papermill": {
     "duration": 0.006227,
     "end_time": "2025-11-14T16:30:51.431229",
     "exception": false,
     "start_time": "2025-11-14T16:30:51.425002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#// plotting code from google ai https://www.google.com/search?client=firefox-b-1-d&q=plot+tensorflow+model+history\n",
    "# then modified\n",
    "def plotModelHistory(history,modelNum = -999):\n",
    "    plt.subplot(211)\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'],label=\"Train\")\n",
    "    plt.plot(history.history['val_loss'],label=\"Validation\")\n",
    "    plt.title(f'Model {modelNum} loss and accuracy')\n",
    "    plt.ylabel('Loss')\n",
    "    # plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "    plt.subplot(212)\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(history.history['binary_accuracy'],label=\"Train\")\n",
    "    plt.plot(history.history['val_binary_accuracy'],label=\"Validation\")\n",
    "    # plt.title(f'Model {modelNum} accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "812ceb14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T16:30:51.434931Z",
     "iopub.status.busy": "2025-11-14T16:30:51.434752Z",
     "iopub.status.idle": "2025-11-14T16:30:51.451529Z",
     "shell.execute_reply": "2025-11-14T16:30:51.451170Z"
    },
    "papermill": {
     "duration": 0.019873,
     "end_time": "2025-11-14T16:30:51.452419",
     "exception": false,
     "start_time": "2025-11-14T16:30:51.432546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model1(SmartPixModel):\n",
    "    def __init__(self,\n",
    "            tfRecordFolder: str = \"/local/d1/smartpixML/filtering_models/shuffling_data/all_batches_shuffled_bigData_try2/filtering_records16384_data_shuffled_single_bigData/\",\n",
    "            nBits: list = None, # just for fractional bits, integer bits\n",
    "                                ## number of bits is the number of bits for each quantized model and then\n",
    "                                ## run training should make one model for each bit size\n",
    "            loadModel: bool = False,\n",
    "            modelPath: str = None, # Only include if you are loading a model\n",
    "                        # dropout_rate: float = 0.1,\n",
    "            initial_lr: float = 1e-3,\n",
    "            end_lr: float = 1e-4,\n",
    "            power: int = 2,\n",
    "            bit_configs = [(16, 0), (8, 0), (6, 0), (4, 0), (3, 0), (2, 0)]  # Test 16, 8, 6, 4, 3, and 2-bit quantization\n",
    "            ): \n",
    "        self.tfRecordFolder = tfRecordFolder\n",
    "        self.modelName = \"Model1\" # for other models, e.g., Model 1, Model 2, etc.\n",
    "        # self.model = None\n",
    "        self.histories = {}\n",
    "        self.models = {\"Unquantized\": None}\n",
    "        self.bit_configs = bit_configs\n",
    "        for total_bits, int_bits in self.bit_configs:\n",
    "            config_name = f\"quantized_{total_bits}w{int_bits}i\"\n",
    "            self.models[config_name] = None\n",
    "        # self.quantized_model = None\n",
    "        self.hyperparameterModel = None\n",
    "        self.training_generator = None\n",
    "        self.validation_generator = None\n",
    "        self.x_feature_description: list = ['z_global','x_size', 'y_size', 'y_local']\n",
    "        # Learning rate parameters\n",
    "        self.initial_lr = initial_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.power = power\n",
    "        return\n",
    "     \n",
    "    def makeUnquantizedModel(self):\n",
    "        ## here i will be making a 4-layer neural network \n",
    "        ## Model 1: z-global, x size, y size, y local\n",
    "\n",
    "\n",
    "        ## define the inputs\n",
    "        input1 = tf.keras.layers.Input(shape=(1,), name=\"z_global\")\n",
    "        input2 = tf.keras.layers.Input(shape=(1,), name=\"x_size\")\n",
    "        input3 = tf.keras.layers.Input(shape=(1,), name=\"y_size\")\n",
    "        input4 = tf.keras.layers.Input(shape=(1,), name=\"y_local\")\n",
    "\n",
    "        ## concatenate the inputs into one layer\n",
    "        inputList = [input1, input2, input3, input4]\n",
    "        inputs = tf.keras.layers.Concatenate()(inputList)\n",
    "\n",
    "\n",
    "        ## here i will add the layers \n",
    "\n",
    "        stack1 = tf.keras.layers.Dense(17,activation='relu')(inputs)\n",
    "        stack2 = tf.keras.layers.Dense(20, activation='relu')(stack1)\n",
    "        stack3 = tf.keras.layers.Dense(9, activation='relu')(stack2)\n",
    "        stack4 = tf.keras.layers.Dense(16, activation='relu')(stack3)\n",
    "        stack5 = tf.keras.layers.Dense(18, activation='relu')(stack4)\n",
    "        output = tf.keras.layers.Dense(1,activation='sigmoid')(stack5)\n",
    "\n",
    "        self.models[\"Unquantized\"] = tf.keras.Model(inputs=inputList, outputs=output)\n",
    "\n",
    "\n",
    "    def makeUnquatizedModelHyperParameterTuning(self):\n",
    "        def model_builder(hp):\n",
    "            # ── B) Architecture hyperparams ──────────────────────────────────────────\n",
    "            # separately tune rows and cols\n",
    "\n",
    "            row1nodes      = hp.Int(\"1\",   10, 200, step=10)\n",
    "            row2nodes      = hp.Int(\"2\",   10, 200, step=10)\n",
    "            row3nodes      = hp.Int(\"3\",   10, 200, step=10)\n",
    "            row4nodes      = hp.Int(\"4\",   10, 200, step=10)\n",
    "            row5nodes      = hp.Int(\"5\",   10, 200, step=10)\n",
    "\n",
    "\n",
    "\n",
    "            input1 = tf.keras.layers.Input(shape=(1,), name=\"z_global\")\n",
    "            input2 = tf.keras.layers.Input(shape=(1,), name=\"x_size\")\n",
    "            input3 = tf.keras.layers.Input(shape=(1,), name=\"y_size\")\n",
    "            input4 = tf.keras.layers.Input(shape=(1,), name=\"y_local\")\n",
    "\n",
    "            ## concatenate the inputs into one layer\n",
    "            inputList = [input1, input2, input3, input4]\n",
    "            inputs = tf.keras.layers.Concatenate()(inputList)\n",
    "\n",
    "\n",
    "            ## here i will add the layers \n",
    "\n",
    "            # layer 1\n",
    "            x = tf.keras.layers.Dense(row1nodes,activation='relu')(inputs)\n",
    "            x = tf.keras.layers.Dense(row2nodes, activation='relu')(x)\n",
    "            x = tf.keras.layers.Dense(row3nodes, activation='relu')(x)\n",
    "            x = tf.keras.layers.Dense(row4nodes, activation='relu')(x)\n",
    "            x = tf.keras.layers.Dense(row5nodes, activation='relu')(x)\n",
    "            output = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "            model = tf.keras.Model(inputs=inputList, outputs=output)\n",
    "\n",
    "            model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"binary_crossentropy\",\n",
    "            metrics=[\"binary_accuracy\"],\n",
    "            run_eagerly  = True \n",
    "            )\n",
    "            return model\n",
    "\n",
    "        tuner = kt.RandomSearch(\n",
    "        model_builder, \n",
    "        objective           = \"val_binary_accuracy\",\n",
    "        max_trials          = 120,\n",
    "        executions_per_trial = 2,\n",
    "        project_name        = \"new_hyperparam_search\"\n",
    "        )\n",
    "\n",
    "        tuner.search(\n",
    "            self.training_generator,\n",
    "            validation_data = self.validation_generator,\n",
    "            epochs          = 110,\n",
    "            callbacks       = [\n",
    "                EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        print(tuner.get_best_hyperparameters(num_trials=1)[0])\n",
    "        print(\"Best hyperparameters:\")\n",
    "        for i in range(1, 6):\n",
    "            print(f\"row{i}nodes:\", best_hp.get(str(i)))\n",
    "                \n",
    "        return tuner\n",
    "     \n",
    "\n",
    "\n",
    "    def makeQuantizedModel(self):\n",
    "        for total_bits, int_bits in self.bit_configs:\n",
    "            config_name = f\"quantized_{total_bits}w{int_bits}i\"\n",
    "        \n",
    "        \n",
    "            print(f\"Building {config_name} model...\")\n",
    "            self.makeQuantizedModel_withBits(total_bits=total_bits,int_bits=int_bits)\n",
    "    def makeQuantizedModel_withBits(self, total_bits = 8,int_bits =0):\n",
    "        \"\"\"\n",
    "        Build & compile your QKeras model with the given number of integer bits.\n",
    "        \"\"\"\n",
    "        tf.keras.backend.clear_session()\n",
    "        # inputs\n",
    "        input1 = tf.keras.layers.Input(shape=(1,), name=\"z_global\")\n",
    "        input2 = tf.keras.layers.Input(shape=(1,), name=\"x_size\")\n",
    "        input3 = tf.keras.layers.Input(shape=(1,), name=\"y_size\")\n",
    "        input4 = tf.keras.layers.Input(shape=(1,), name=\"y_local\")\n",
    "        x = tf.keras.layers.Concatenate()([input1, input2, input3, input4])\n",
    "\n",
    "        ## I want to try this with 1 int bit and 7 fractional\n",
    "        ## I want to try this with 0 int bit and 7 fractional\n",
    "        \n",
    "        # layer 1\n",
    "        x = QDense(\n",
    "            17,\n",
    "            kernel_quantizer=quantized_bits(total_bits, int_bits, alpha=1),\n",
    "            bias_quantizer=quantized_bits(total_bits, int_bits, alpha=1),\n",
    "            #kernel_regularizer=tf.keras.regularizers.L1L2(0.0001),\n",
    "            ## adds sum of the activations squared to the loss function \n",
    "            #activity_regularizer=tf.keras.regularizers.L2(0.0001),\n",
    "        )(x)\n",
    "        x = QActivation(\n",
    "            activation=quantized_relu(total_bits, int_bits),\n",
    "            name=\"q_relu1\"\n",
    "        )(x)\n",
    "\n",
    "        # layer 2 (example—you can tweak per‐layer bits)\n",
    "        x = QDense(\n",
    "            20,\n",
    "            kernel_quantizer=quantized_bits(total_bits, int_bits, alpha=1),\n",
    "            bias_quantizer=quantized_bits(total_bits, int_bits, alpha=1),\n",
    "            #kernel_regularizer=tf.keras.regularizers.L1L2(0.0001),\n",
    "            ## adds sum of the activations squared to the loss function \n",
    "            #activity_regularizer=tf.keras.regularizers.L2(0.0001),\n",
    "        )(x)\n",
    "        x = QActivation(\n",
    "            activation=quantized_relu(total_bits, int_bits),\n",
    "            name=\"q_relu2\"\n",
    "        )(x)\n",
    "\n",
    "        # layer 3\n",
    "        x = QDense(\n",
    "            9,\n",
    "            kernel_quantizer=quantized_bits(total_bits, int_bits, alpha=1),\n",
    "            bias_quantizer=quantized_bits(total_bits, int_bits, alpha=1),\n",
    "            #kernel_regularizer=tf.keras.regularizers.L1L2(0.0001),\n",
    "            ## adds sum of the activations squared to the loss function \n",
    "            #activity_regularizer=tf.keras.regularizers.L2(0.0001),\n",
    "        )(x)\n",
    "        x = QActivation(\n",
    "            activation=quantized_relu(total_bits, int_bits),\n",
    "            name=\"q_relu3\"\n",
    "        )(x)\n",
    "\n",
    "        # layer 4\n",
    "        x = QDense(\n",
    "            16,\n",
    "            kernel_quantizer=quantized_bits(total_bits, int_bits, alpha=1),\n",
    "            bias_quantizer=quantized_bits(total_bits, int_bits, alpha=1),\n",
    "            #kernel_regularizer=tf.keras.regularizers.L1L2(0.0001),\n",
    "            ## adds sum of the activations squared to the loss function \n",
    "            #activity_regularizer=tf.keras.regularizers.L2(0.0001),\n",
    "        )(x)\n",
    "        x = QActivation(\n",
    "            activation=quantized_relu(total_bits, int_bits),\n",
    "            name=\"q_relu4\"\n",
    "        )(x)\n",
    "\n",
    "        # layer 5\n",
    "        x = QDense(\n",
    "            8,\n",
    "            kernel_quantizer=quantized_bits(total_bits, int_bits, alpha=1),\n",
    "            bias_quantizer=quantized_bits(total_bits, int_bits, alpha=1),\n",
    "            #kernel_regularizer=tf.keras.regularizers.L1L2(0.0001),\n",
    "            ## adds sum of the activations squared to the loss function \n",
    "            #activity_regularizer=tf.keras.regularizers.L2(0.0001),\n",
    "        )(x)\n",
    "        x = QActivation(\n",
    "            activation=quantized_relu(total_bits, int_bits),\n",
    "            name=\"q_relu5\"\n",
    "        )(x)\n",
    "\n",
    "        # output\n",
    "        x = QDense(\n",
    "            1,\n",
    "            kernel_quantizer=quantized_bits(total_bits, int_bits, alpha=1),\n",
    "            bias_quantizer=quantized_bits(total_bits, int_bits, alpha=1),\n",
    "            #kernel_regularizer=tf.keras.regularizers.L2(0.0001),\n",
    "        )(x)\n",
    "        out = QActivation(\"smooth_sigmoid\")(x)\n",
    "        config_name = f\"quantized_{total_bits}w{int_bits}i\"\n",
    "        self.models[config_name] = tf.keras.Model(inputs=[input1, input2, input3, input4], outputs=out)\n",
    "\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def runHyperparameterTuning(self):\n",
    "        raise NotImplementedError(\"Subclasses should implement this method.\")\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a5286b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T16:30:51.455766Z",
     "iopub.status.busy": "2025-11-14T16:30:51.455415Z",
     "iopub.status.idle": "2025-11-16T03:46:46.306830Z",
     "shell.execute_reply": "2025-11-16T03:46:46.306325Z"
    },
    "papermill": {
     "duration": 126954.856083,
     "end_time": "2025-11-16T03:46:46.309776",
     "exception": false,
     "start_time": "2025-11-14T16:30:51.453693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data from: /local/d1/smartpixML/filtering_models/shuffling_data/all_batches_shuffled_bigData_try2/filtering_records16384_data_shuffled_single_bigData//tfrecords_train/\n",
      "Loading validation data from: /local/d1/smartpixML/filtering_models/shuffling_data/all_batches_shuffled_bigData_try2/filtering_records16384_data_shuffled_single_bigData//tfrecords_validation/\n",
      "Using batch_size=16384 to match TFRecord format\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training generator length: 157\n",
      "Validation generator length: 40\n",
      "Reloading Tuner from ./new_hyperparam_search/tuner0.json\n",
      "<keras_tuner.src.engine.hyperparameters.hyperparameters.HyperParameters object at 0x7f007407e6e0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_tuner.src.tuners.randomsearch.RandomSearch at 0x7f007589be80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = Model1()\n",
    "SmartPixModel.loadTfRecords(m1)\n",
    "m1.makeUnquatizedModelHyperParameterTuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "641fdd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<keras.src.engine.functional.Functional object at 0x7f713ac5f490>]\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.24\n"
     ]
    }
   ],
   "source": [
    "print(m1.tuner.get_best_models())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj_qkeras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 126959.958377,
   "end_time": "2025-11-16T03:46:47.731730",
   "environment_variables": {},
   "exception": null,
   "input_path": "hyperparam.ipynb",
   "output_path": "results.ipynb",
   "parameters": {},
   "start_time": "2025-11-14T16:30:47.773353",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
